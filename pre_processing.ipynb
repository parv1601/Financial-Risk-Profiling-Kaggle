{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d5d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f826f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Data Shape: (204277, 18)\n",
      "Original Test Data Shape: (51070, 17)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train_updated.csv')\n",
    "test_df = pd.read_csv('data/test_updated.csv')\n",
    "\n",
    "print(f\"Original Training Data Shape: {train_df.shape}\")\n",
    "print(f\"Original Test Data Shape: {test_df.shape}\")\n",
    "\n",
    "# Separate IDs and Target variable\n",
    "train_profile_id = train_df['ProfileID']\n",
    "train_target = train_df['RiskFlag']\n",
    "test_profile_id = test_df['ProfileID']\n",
    "\n",
    "# Drop ProfileID and RiskFlag (from train_df) for core processing\n",
    "train_df_proc = train_df.drop(columns=['ProfileID', 'RiskFlag'])\n",
    "test_df_proc = test_df.drop(columns=['ProfileID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b836792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"Creates SumToEarningsRatio and DebtLoad features.\"\"\"\n",
    "    # Ratio of Requested Sum to Annual Earnings\n",
    "    df['SumToEarningsRatio'] = df['RequestedSum'] / df['AnnualEarnings']\n",
    "    # Debt Load feature combining RequestedSum and DebtFactor\n",
    "    df['DebtLoad'] = df['RequestedSum'] * df['DebtFactor']\n",
    "    return df\n",
    "\n",
    "train_df_proc = feature_engineering(train_df_proc.copy())\n",
    "test_df_proc = feature_engineering(test_df_proc.copy())\n",
    "\n",
    "# Identify all numerical columns\n",
    "numerical_cols = train_df_proc.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08881662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers_iqr(df, column, lower_bound, upper_bound):\n",
    "    \"\"\"Applies capping/flooring based on provided bounds.\"\"\"\n",
    "    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "# Calculate bounds ONLY on the training data\n",
    "bounds_train = {}\n",
    "for col in numerical_cols:\n",
    "    Q1 = train_df_proc[col].quantile(0.25)\n",
    "    Q3 = train_df_proc[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    bounds_train[col] = (lower, upper)\n",
    "\n",
    "# Apply capping to both train and test using train bounds\n",
    "for col in numerical_cols:\n",
    "    lower_bound, upper_bound = bounds_train[col]\n",
    "    train_df_proc = cap_outliers_iqr(train_df_proc, col, lower_bound, upper_bound)\n",
    "    test_df_proc = cap_outliers_iqr(test_df_proc, col, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bdf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = train_df_proc.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Concatenate for consistent encoding across train and test\n",
    "combined_df = pd.concat([train_df_proc, test_df_proc], ignore_index=True)\n",
    "combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate back into processed train and test sets\n",
    "train_processed_df = combined_df_encoded.iloc[:len(train_df)].copy()\n",
    "test_processed_df = combined_df_encoded.iloc[len(train_df):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff68ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler only on the training data's numerical columns\n",
    "train_processed_df[numerical_cols] = scaler.fit_transform(train_processed_df[numerical_cols])\n",
    "\n",
    "# Transform test data using the fitted scaler\n",
    "test_processed_df[numerical_cols] = scaler.transform(test_processed_df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76829aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_df.insert(0, 'ProfileID', train_profile_id)\n",
    "train_processed_df['RiskFlag'] = train_target\n",
    "\n",
    "# Correctly re-attach ProfileID to the test data (must reset index first)\n",
    "test_processed_df = test_processed_df.reset_index(drop=True)\n",
    "test_processed_df.insert(0, 'ProfileID', test_profile_id)\n",
    "\n",
    "# Save the final processed DataFrames\n",
    "train_processed_df.to_csv('train_processed_parv.csv', index=False)\n",
    "test_processed_df.to_csv('test_processed_parv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
